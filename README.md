# ğŸ“ AdHocâ€‘LM
*A transformerâ€‘based language model built from scratch, specialized for diplomacy, resolutions, and debate (work in progress).*

---

## ğŸŒ Overview
**AdHocâ€‘LM** is my ongoing project to **build a large language model from scratch in PyTorch**.  

The longâ€‘term goal is to create a model specialized in **diplomatic communication, resolution drafting, and debate simulation**.  
Right now, the project is in **Phase 1: Core LLM Build**, where Iâ€™m implementing the fundamental transformer components and training a **Miniâ€‘AdHocâ€‘LM** on small datasets (e.g., Shakespeare, TinyStories, WikiText).  

This repo will be updated as I progress through each phase:  
1. Core LLM Build (now)  
2. Scaling & Engineering  
3. Domain Specialization  
4. Application & RAG Integration  
5. Visualization & Portfolio Polish  

---

## ğŸ”¹ Current Status (Phase 1)
- âœ… Repository initialized  
- âœ… Environment set up (PyTorch, CUDA, Hugging Face)  
- âœ… Tokenizer + embeddings implemented  
- ğŸš§ Working on: Multiâ€‘head attention module  

**Next Steps:**  
- Implement feedforward layers & normalization  
- Add causal masking for autoregressive training  
- Build training loop  
- Train **Miniâ€‘AdHocâ€‘LM** (5â€“10M parameters) on Shakespeare dataset  

---

## ğŸ“‚ Project Structure (so far)
